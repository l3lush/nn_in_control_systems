{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, loss_func=BinaryCrossEntropyLossWithBackprop()):\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.loss_func = loss_func\n",
    "        self.grads = []\n",
    "\n",
    "    def add_layer(self,layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dout):\n",
    "        self.clear_grad_param()\n",
    "        for layer in reversed(self.layers):\n",
    "            dout, grad = layer.backward(dout)\n",
    "            self.grads.append(grad)\n",
    "        return self.grads\n",
    "\n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss = self.loss_func.forward(y, out)\n",
    "        dout = self.loss_func.backward(y, out)\n",
    "        grads = self.backward(dout)\n",
    "        return loss, grads\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        return X\n",
    "\n",
    "    def dispGradParam():\n",
    "        print(self.grads)\n",
    "    \n",
    "    def clear_grad_param(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8595de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5299229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor([1, 1, 1, 0, 0], dtype=torch.float32)\n",
    "y_pred = torch.tensor([0.99, 0.9, 0.8, 0.5, 0.2], requires_grad=True, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f75873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bceloss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8198df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = bceloss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d12cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbebf2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9900, 0.9000, 0.8000, 0.5000, 0.2000], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49cca3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2020, -0.2222, -0.2500,  0.4000,  0.2500])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c66846c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anana\\AppData\\Local\\Temp\\ipykernel_26684\\625247546.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  a.grad\n"
     ]
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.randn(in_size, out_size) / np.sqrt(in_size + out_size/ 2.)\n",
    "        self.params = [self.W]\n",
    "        self.gradW = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X.copy()\n",
    "        output = X.dot(self.W)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class ReLULayer():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return np.maximum(X, 0)\n",
    "    \n",
    "    \n",
    "class CrossEntropyLoss():\n",
    "    def forward(self, y_true, y_pred):\n",
    "        loss = -(1 / m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class LinearLayerWithBackprop(LinearLayer):\n",
    "    def backward(self, dout):\n",
    "        self.gradW = self.X.T.dot(dout)\n",
    "        self.gradInput = dout.dot(self.W.T)\n",
    "        return self.gradInput, [self.gradW]\n",
    "    \n",
    "    \n",
    "class ReLULayerWithBackprop(ReLULayer):   \n",
    "    def backward(self, dout):\n",
    "        self.gradInput = dout.copy()\n",
    "        self.gradInput[self.X <= 0] = 0\n",
    "        return self.gradInput, []\n",
    "    \n",
    "    \n",
    "class BinaryCrossEntropyLossWithBackprop(CrossEntropyLoss):\n",
    "    def backward(self, y_true, y_pred):\n",
    "        m = len(y_true)\n",
    "        loss = (1 / m) * (-(y_true / y_pred) + ((1 - y_true) / (1 - y_pred)))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4e8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cbfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.randn(in_size, out_size)\n",
    "        self.params = [self.W]\n",
    "        self.gradW = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X.copy()\n",
    "        output = X.dot(self.W)\n",
    "        return output\n",
    "\n",
    "class ReLULayer():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X.copy()\n",
    "        output = np.maximum(X, 0)\n",
    "        return output\n",
    "\n",
    "class BinaryCrossEntropyLoss():\n",
    "    def forward(self, y_true, y_pred):\n",
    "        n = len(y_true)\n",
    "        loss = - (1 / n) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "class LinearLayerWithBackprop(LinearLayer):\n",
    "    def backward(self, dout):\n",
    "        self.gradW = self.X.T.dot(dout)\n",
    "        self.gradInput = dout.dot(self.W.T)\n",
    "        return self.gradInput, [self.gradW]\n",
    "\n",
    "class ReLULayerWithBackprop(ReLULayer):   \n",
    "    def backward(self, dout):\n",
    "        self.gradInput = dout.copy()\n",
    "        self.gradInput[self.X <= 0] = 0\n",
    "        return self.gradInput, []\n",
    "\n",
    "class BinaryCrossEntropyLossWithBackprop(BinaryCrossEntropyLoss):\n",
    "    def backward(self, y_true, y_pred):\n",
    "        m = len(y_true)\n",
    "        loss = (1 / m) * (-(y_true / y_pred) + ((1 - y_true) / (1 - y_pred)))\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20616dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_fall_2022_venv",
   "language": "python",
   "name": "nn_fall_2022_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
